## Tips
1. 要写文章就写没有人写过的，或是别人写过的，但我能写得更好的。最好的SEO就是独一份；
2. 关注有价值的东西。包括市场需求、技术趋势。学习技术要考虑两个问题：1）这个技术解决的是什么问题？为什么别的技术做不到？2）为什么是这样解决的？有没有更好的方式？
3. 一个新的技术顺应趋势，其背后肯定有大公司推动
4. 会挣钱的人一定是会投资的人。你把你的时间投资到了哪里，决定了你将成为什么样的人

## 如何做一个有技术领导力的人

### What
1. 能够发现问题
2. 能够提供解决问题的思路和方案
3. 能够做出正确的技术决定
4. 能够用更优雅、更简单的方式解决问题
5. 能够提高代码或者软件的扩展性、重用性和可维护性
6. 能够正确管理团队。发挥每个人潜力
7. 创新能力

> 总是在提供解决问题的思路和方案的人才是有技术领导力的人

### How
#### 基础扎实
1. C语言、汇编最底层的与OS接触的基础
2. 编程范式。面向对象、函数式编程等
3. 算法数据结构
4. 系统部分：计算机系统原理、OS、网络、数据库、分布式技术架构
#### 学习能力
1. 要学第一手的资料
2. 与牛人多接触

#### 坚持做正确的事情
1. 时间管理
2. 效率、自动化
3. 技术驱动，多了解其他新兴领域

#### 不断提高对自己要求
1. Google自我评分卡
2. 敏锐的技术嗅觉能力
3. 学以致用，强调实践
4. 永远在编程，对细节敏感

## 技术发展要求

1. 有没有一个活跃的社区
2. 有没有一个工业化的标准
3. 有没有一个或多个杀手锏应用
4. 学习难度是否低，上手是否容易
5. 有没有一个不错的提高开发效率的框架
6. 是否有一个或者多个大公司作为后盾
7. 有没有解决软件开发中的痛点

## 程序异常处理

大体的，我们可以将异常分为如下三类：
1. 资源的错误。例如没有权限，没有足够内存，网络故障等；
2. 程序的错误。例如空指针，非法参数。对于这类错误，由于是我们自己程序的错误，我们要记录下来，写入日志，最好触发监控系统报警；
3. 用户错误。如用户非法输入，基本上是在用户API层出现的错误。对于这类错误，我们需要向用户报错让用户自己处理修正输入。此外，我们可以统计相应错误率，有利于我们改善软件或是侦测是否有恶意的用户请求

通常，对于异常的处理，我们有异常捕捉（try-catch-finally）或者错误返回码两种方法处理。我们可以按照如下原则进行划分：
1. 对于我们并不期望发生的事情，我们可以使用异常捕捉；
2. 对于我们觉得可能会发生的，使用错误码

因此，对于上述的三种异常，程序中的异常，使用异常捕捉；用户错误，使用错误码；对于资源错误，则根据具体情况判断。

当然，有时候这两种异常处理方式并不是等价替代的：
1. 异常捕捉只能在同步情况下使用，在异步模式下，无法使用抛异常的方式解决，需要通过检查子进程退出码或者回调函数解决。https://www.cnblogs.com/yangfanexp/p/7594557.html Java的多线程下的异常处理
2. 在分布式情况下，调用远程服务只能看错误码，比如HTTP的错误码

当然，使用错误码还是异常捕捉主要还是看我们的错误处理流程以及代码组织怎么写更加清楚

## 异步编程错误处理最佳实践

TODO 

Promise模式

## 时间管理

学会说不
1. 给出一个你可以做到的方案，而不是把对方的方案直接回绝掉
2. 我不说我不能完全满足你，但我说可以部分满足你
3. 我不能说不，但是有条件的说是。而且，我要把你给我的压力再反过来还给你，看似我给了需求方选择，实际上，我掌握了主动

Tips：
1. 我可以加班加点完成，但是我不能很好保证质量，有bug你得认，而且事后你要给我一个月时间还债
2. 我可以加班加点，还能保证质量，但我没办法完成这么多需求，能不能减少一点？
3. 我可以保质保量完成所有需求，但是能不能多给我两周时间？

> 关于开会：开会不是讨论问题，而是讨论方案，开会不是要有议题，而是要有议案

### 投资自己的时间

1. 花时间学习基础知识，花时间读文档。避免太多时间花在差错上
2. 花时间在解放生产力的事上。自动化、可配置、可重用。做好整理
3. 花时间在让自己成长的事情上
4. 花时间在建立高效的环境上

## 故障处理最佳实践：应对故障

> 出现故障时，最重要的不是debug故障，而是尽可能减少故障影响范围，并尽可能快地修复问题

通常有如下几种手段来恢复系统：
1. 重启和限流
2. 回滚操作。一般用于解决新代码引入的bug，快速回滚到之前版本
3. 降级操作。并不是所有代码都能回滚，无法回滚时，就要降级。需要挂一个停止服务的故障公告，主要是不要把事态扩大
4. 紧急更新。发现问题并修正后，迅速上线。这需要强大的自动化系统，尤其是自动化测试和自动化发布系统


### 故障前准备工作

1. 以用户功能为索引的服务和资源的全视图。与自动化监控系统整合，这样我们就能够很容易找到用户整个操作涉及到的所有服务，也就容易找到处理故障的路径
2. 为各个服务制定关键指标，以及一套运维流程和工具，包括应急方案。如redis，怎么检查其是否有问题，怎么查看其健康和运行状态？哪些是关键指标，面对常见故障的怎么应对，服务需要回滚了怎么操作等
3. 设定故障等级。主要是为了确定该故障等级要牵扯进多大规模的人员来处理。
4. 故障演练。
5. 灰度发布

## 故障处理最佳实践：故障改进

亚马逊故障复盘过程：
- 故障处理的整个过程。像log一样，详细记录几点几分干了什么事，把故障从发生到解决的所有细节过程都记录下来
- Ask 5 Whys。为什么会出现该故障
- 故障后续整个计划。针对上述的why，说明后续如何举一反三从根本上解决这些问题

根除问题本质：
1. 举一反三解决当下的故障。为自己赢得更多时间
2. 简化复杂、不合理的技术架构、流程和组织。
3. 全面改善和优化整个系统，包括组织。

很多问题出了又出，换着花样出，大多数情况下是因为这个公司的系统架构太过复杂和混乱，以至于你不可能在这样的环境下干干净净解决所有问题

## Git协同工作流

常用的工作流有：
1. 中心式协同工作流
2. 功能分支协同工作流
3. GitFlow协同工作流
4. Github工作流
5. Gitlab工作流

## 分布式系统架构的本质

分布式架构与单体架构的异同：
![](/img/post/zuoertingfeng/distributed_trait.png)

### 分布式系统难点

#### 异构系统的不标准问题
1. 软件和应用不标准
2. 通信协议不标准
3. 数据格式不不标准
4. 开发和运维的过程和方法不标准

#### 系统架构中的服务型依赖性问题
- 如果非关键业务被关键业务所依赖，会导致非关键业务变成一个关键业务
- 服务依赖链中，出现“木桶短板效应”，整个系统由最差的那个服务决定

#### 故障发生的概率大
- 出现故障不可怕，故障恢复时间过长才可怕
- 出现故障不可怕，故障影响面过大才可怕

#### 多层架构的运维复杂度更大
通常系统架构分成四层：
1. 基础层：就是我们的机器、网络和存储设备
2. 平台层：就是我们的中间件层，Tomcat,Mysql,Redis,Kafka等
3. 应用层：就是我们的业务软件，比如各种功能的服务
4. 接入层：就是接入用户的网关、负载均衡或是CDN、DNS这样的东西

- 任何一层的问题都会导致整体问题
- 没有统一的视图和管理，导致运维被割裂开来，造成更大的复杂度

### 分布式系统的技术栈

#### 提高架构的性能
![](/img/post/zuoertingfeng/distributed_performance.png)

#### 提高系统可用性
![](/img/post/zuoertingfeng/distributed_avaliable.png)

#### 分布式系统的关键技术
- 服务治理：服务拆分、服务调用、服务关键度定义等，其最大意义是需要把服务间的依赖关系、服务调用链，以及关键的服务梳理出来，并对这些服务进行性能和可用性的管理
- 架构软件管理。不同服务之间也存在兼容性问题，需要有架构版本管理、生命周期管理等
- DevOps。需要快速更新服务，需要更加自动化的测试发布机制，包括环境构建、持续集成、持续部署等
- 自动化运维。如自动伸缩、故障迁移、配置管理
- 资源调度管理。存储、网路等资源调度、隔离
- 整体架构监控。通过监控查看各个服务之间的健康度
- 流量控制。负载均衡、服务路由、熔断、降级、限流、灰度发布等

总之，可以分为五个关键技术
- 全栈系统监控
- 服务、资源调度
- 流量调度
- 状态、数据调度
- 开发和运维的自动化

![](/img/post/zuoertingfeng/distributed_keys.png)

### 全栈监控
一个好的监控系统需要在多层做好监控

![](/img/post/zuoertingfeng/monitor.png)

此外，各层之间应该约定好监控的标准化
- 日志数据结构化
- 监控数据格式标准化
- 统一的监控平台
- 统一的日志分析

#### 如何做出一个好的监控系统
- 服务调用链跟踪。这个监控系统应该从对外的API开始，然后将后台的实际服务关联起来，再进一步将这个服务的依赖服务关联，直到最后一个服务。最佳实践：**Zipkin**
- 服务调用时长分布。有助于我们知道耗时最长的任务是什么
- 服务的TOP N视图。
- 数据库操作关联
- 服务资源跟踪。无论是物理机、虚拟机、容器等，我们需要把其CPU/MEM/IO/DISK/NETWORK资源关联起来

有了这些之后，我们就能很容易发现一旦某个服务出现了问题，其会影响到哪个对外API等

我们也可以根据这些问题，针对性的做出改进措施
- 一旦发现某个服务过慢是因为CPU使用过多，那么我们就可以做弹性伸缩
- 一旦发现某个服务过慢是因为MYSQL出现了慢查询，那么做弹性伸缩不能解决，应该做流量限制，或是降级操作

### 服务调度

- 根据业务理解，定义**服务关键程度**
- 梳理服务依赖关系。**不能出现环**
- 管理服务状态（Provision, Ready, Run, Update, Rollback, Scale, Destroy, Failed），以及生命周期管理
- 管理好整个架构的版本，例如不同服务之间可能只能依赖特定版本的服务，因此需要梳理一个清单，确定能够兼容的版本，做到发布与回滚的时候能够快速
- 服务状态的维持和拟合。在不同状态切换时，例如弹性伸缩和故障迁移，建立完善的切换机制
- 服务工作流和编排

### 流量与数据调度

#### 流量调度的主要功能

1. 依据系统运行的情况，自动地进行流量调度，在无需人工干预的情况下，提升整个系统的稳定性
2. 让系统应对突发事件时，在弹性计算扩容的较长事件窗口内或底层资源殆尽的情况下，保护系统平稳运行

需要完成如下几方面的事情
- **服务流控**。服务发现、服务路由、服务降级、服务熔断、服务保护
- **流量控制**。负载均衡、流量分配、流量控制、异地容灾
- **流量管理**。协议转换、请求校验、数据缓存、数据计算

#### 数据调度

- 对于应用层的分布式事务一致性，只有**两阶段提交**这样的方式
- 而底层存储可以解决这个问题的方式是通过一些Paxos，Raft这样的算法和模型来解决

### 分布式系统架构经典资源与数据调度相关论文

见专栏。


## 分布式系统弹力设计

### 认识故障和弹力设计
所谓弹力设计指**如何提高系统可用性**，以应对故障

#### 可用性测量

![](/img/post/zuoertingfeng/availability_cal.jpg)

其中，
- MTTF指Mean Time to Failure，平均故障前时间。即系统平均能够正常运行多久时间才会发生一次故障
- MTTR指Mean Time to Recovery，平均修复时间，即从故障出现到故障修复的这段时间
- 因此，MTTF时间越长，MTTR时间越短，那么该系统可用性行业就越高。

我们平常所说的几个9，就是通过这个公式计算得到的。
![](/img/post/zuoertingfeng/availability_9.png)

#### 故障原因

宕机原因可以大致分为两大类：

**无计划的**
- 系统级故障，包括主机、OS、中间件、数据库、网络等
- 数据和中介的故障，包括人员误操作、硬盘故障
- 自然灾害、人为破坏以及供电问题

**有计划的**
- 日常任务：备份，容量规划，用户和安全管理
- 运维相关：数据库维护、应用维护、OS维护等
- 升级相关：各种软件硬件的升级

> 总之，故障是不可避免的，而且是常见的。我们要做的是，当故障发生的时候，我们要尽量将故障影响的范围缩小以及能够在短时间内修复

### 隔离设计

> 隔离的目的在于当故障发生的时候，将影响范围降到最小

系统隔离设计一般可以分为两种：

#### 按服务的种类做分离

![](/img/post/zuoertingfeng/service_bulkheads.png)
每个服务从接入层再到数据层完全隔离。常用的微服务就是这种模式。从物理上来说，一个服务的故障不会影响到其他服务。然而存在如下问题：
- 如果某个业务需要跨多个服务调用，过多的远程调用使得性能降低。
- 同样，某业务如果跨多个服务，那么当其中一个服务故障时，会使得这个业务无法继续进行
- 如果多个服务之间需要进行事务控制，那么需要引入”二阶段提交“这样的方案，引入复杂度

#### 按用户请求分离
![](/img/post/zuoertingfeng/user_bulkheads.png)
这里，将用户分为不同组，并把后端的同一个服务根据这些不同的组分为不同的实例。让同一个服务对于不同的用户进行冗余和隔离。这样当某组实例挂掉了，只会影响到部分用户

可以联想到财付通内部的订单实现，就是根据用户ID去分组。所谓的”多租户“模式就是如此，对于一些比较大的客户，我们可以为他们设置专门独立的服务实例，或是服务集群与其他客户隔离开来，对于一些比较小的用户来说，可以让他们共享一个服务实例，达到节省资源的目的。

按用户请求分离带来的问题主要在于权衡，如果完全隔离，资源使用上比较浪费，如果共享，又会导致程序设计的复杂。一个折中的方案是，服务做成共享的，而数据通过分区来隔离

