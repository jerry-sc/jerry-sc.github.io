---
layout: post
title: Linux性能优化
date: 2019-01-21
author: Jerry
header-style: text
catalog: true
tags:
 - Linux
---

# CPU

## 平均负载

我们可以通过`uptime`命令获取系统的平均负载
```shell
> uptime
14:58:54 up 432 days,  1:11,  1 user,  load average: 0.00, 0.00, 0.00
```
这里需要关注的是最后三列，分别表示过去1分钟、5分钟、15分钟的平均负载。

**注意：平均负载不等于CPU使用率**

> 平均负载是指单位时间内，系统处于**可运行状态**和**不可中断状态**的平均**进程数**。

- 可运行状态进程包括**正在使用CPU**的进程以及正在**等待CPU**的就绪进程，对应进程状态R
- 不可中断进程是指正处于内核态关键流程中的进程，并不能被中断。对应进程状态D（Disk Sleep）。

### 不可中断与可中断

OS中分为两种中断，可中断与不可中断，分别对应的进程状态为S与D。
- 处于可中断的进程通常因为**等待某某事件发生（如等待socket连接，等待信号量）**而被挂起。而当这些等待事件发生时，这些进程就会被唤醒。
- 处于不可中断的原因一般如上面所述，进程处于内核态关键流程中，一旦被中断，会引发错误。通常而言，处于该状态的时间较为短暂。比如进程调用read系统调用对某个设备文件进行读操作，而read系统调用最终执行到对应设备驱动代码，并与对应的物理设备进行交互。这时就需要对进程进行保护，以免被中断后，**造成设备陷入不可控状态**。可见不可中断是对系统进程与硬件设备的保护。

按照我的理解，仍旧以read系统调用举例。当CPU执行与设备驱动的内核代码时，即当执行设备控制的时候，这时进程处于D状态。而当控制代码处理完毕后，IO设备开始工作后，即不需要CPU再进行干预时，这时进程处于S，休眠状态，等待DMA数据传送完毕。当数据传输完毕，发送中断后，然后进程再次回到R状态

### 进程状态
[Linux进程状态解析之R、S、D、T、Z、X](https://www.cnblogs.com/YDDMAX/p/4979878.html)
- R
- S
- D
- Z是zombie的缩写，表示僵尸进程，也就是进程实际上已经结束了，但是父进程还没有回收它的资源（如进程的描述符、PID等）
- I是Idle缩写，表示空闲状态，用在不可中断睡眠的内核线程上，该类进程没有任何负载。
- T或t，表示进程处于暂停或者跟踪状态，当我们用gdb调试一个进程时，进程就会处于该状态
- X表示进程已消亡，也就不会再top或者ps命令中看到

通过`uptime`命令查看系统平均负载时，我们更应该关注的是**整体趋势变化**，而不是某一个值的大小。作者建议**当平均负载高于CPU数量70%的时候**，就应该排除为什么负载高的问题了。

### 平均负载与CPU使用率

再次强调下，两者属于不同范畴，要从各自定义出发去理解。平均负载有时与CPU使用率并不一致。
- CPU密集型进程，使用大量CPU会导致平均负载升高，此时这两者一致
- IO密集型进程，等待IO也会导致平均负载升高，但此时CPU使用率不一定高

也就是说，平均负载高有可能是CPU密集引起，或者由于IO密集引起。

### 性能检测工具

#### sysstat
可以使用`sysstat`包下的`mpstat`与`pidstat`
- `mpstat`是一个常用的多核CPU性能分析工具，用来实时查看每个CPU的性能指标，以及所有CPU的平均指标
- `pidstat`是一个常用的进程性能分析工具，用来实时查看进程的CPU、内存、IO以及上下文切换等性能指标

#### dstat

该工具可以同时查看CPU和IO这两种资源的使用情况，便于对比分析

#### strace

常用于跟踪进程的**系统调用**情况

#### perf

用于查看占用CPU的内核函数

#### pstree

查看进程的父子关系

## CPU上下文切换

上下文切换时由于要保持现场，是需要开销的，所以频繁的上下文切换会导致性能低下。

CPU上下文可以分为三类：

### 进程上下文切换

进程的上下文切换相比其他两类切换更加重，更加占用性能。这是因为进程上下文切换不仅包括虚拟内存、栈、全局变量等用户空间的资源，也包括内核堆栈、寄存器等内核空间的状态。

造成进程切换的原因有：
- 时间片用完
- 进程运行资源不足
- 主动sleep挂起
- 高优先级抢占
- 硬件中断执行中断程序

### 线程上下文切换

由于现代OS线程才是调度的基本单位，而进程是资源拥有的基本单位。线程的切换更加轻量与高效。这是因为在同一进程内的线程切换时，只需保存各线程自己的栈和寄存器，而不需保存虚拟内存那些，因为那些是共享的。

当然不同进程内的线程切换时，由于资源不共享，所以先要进行进程切换。

### 中断上下文切换

为了快速响应硬件的事件，中断处理会打断进程的正常调度和执行，转而调用中断处理程序，响应设备事件。

但中断上下文切换并不涉及到进程的用户态，所以即便中断过程打断了一个正处在用户态的进程，也不需要保存和恢复这个进程的虚拟内存、全局变量等用户态资源。于是，相比进程上下文切换来说，也更加轻量。

### 如何查看CPU上下文切换情况

同样可以通过`vmstat`来查看系统情况。
- `cs`列context switch，表示每秒上下文切换次数
- `in`列interrupt，表示每秒中断次数
- 其他字段可以man查询

同样可以通过`pidstat`命令加上`-w`参数来查看进程的上下文切换情况。需要注意的是两列：
- `cswch`表示每秒自愿上下文切换次数，如进程无法获取自愿导致的切换
- `nvcswch`表示每秒非自愿上下文切换次数，如时间片用完等强制发生的切换

注意：`-w`参数下只会显示进程的数据，而不会统计线程切换的数据，可以通过`-wt`来包含进线程数据

此外，我们可以通过读取`/proc/interrupts`文件来查看由于**是哪种中断类型**引起上下文切换次数

### 上下文切换多少次合理

根据作者建议，这个数值取决于CPU性能，如果系统上下文切换次数比较稳定，并且一万以内那么都算正常。如果出现数量级增长，就很可能发生问题。我们可以根据上下文切换的类型，来进一步猜测问题所在，如
- 自愿上下文变多了，说明进程都在等待资源，有可能发生了IO等问题
- 非自愿上下文切换变多了，说明进程都在被强制调度，都在抢CPU，说明CPU成了瓶颈
- 中断次数变多了，说明CPU被中断处理程序占用，需要通过查看`/proc/interrupts`来分析具体的中断类型

## CPU使用率

可以通过以下命令获取系统每个CPU不同场景下的使用率情况，以及整体的使用率情况。这里的数字单位是节拍数。为了维护CPU时间，Linux通过事先定义的节拍率来出发时钟中断，使用全局变量`Jiffies`记录**开机以来**的节拍数。每发生一次时钟中断，就+1.
```shell
> cat /proc/stat | grep ^cpu
cpu  6330964998 0 13176596 8568527998 5856564 0 765996 0 0 0
cpu0 1370073649 0 3186942 2354338860 1493387 0 165629 0 0 0
cpu1 2135161003 0 2985880 1590712563 1560081 0 165150 0 0 0
cpu2 1354084485 0 3602822 2370125834 1556406 0 180915 0 0 0
cpu3 1471645860 0 3400951 2253350740 1246689 0 254302 0 0 0
```

这里记录的不同场景有：
- `user`(us)：代表用户态CPU时间。不包括下面nice时间，但包括guest时间
- `nice`(ni)：代表**低优先级用户态**CPU时间，也就是进程的nice值被调整为1-19之间时的CPU时间。而nice取值为-20到19，数值越大，优先级越低
- `system`(sys)：代表内核态CPU时间
- `idle`(id)：代表空闲时间，但不包括IO等待时间
- `iowait`(wa)：代表等待IO的CPU时间。需要注意下该值的计算方法，iowait+idle才真正表示CPU空闲的时间，即iowait统计的是**当CPU空闲以及至少一个IO进程在运行**的时间，iowait为0并不能代表当前没有IO进程，可能只不过当时还有其他进程在占用cpu。此外反之，当iowait很大时，我们也无法光凭这个值就断定系统有很多进程在等待IO，可能只有一个。
- `irq`(hi)：代表处理硬中断的CPU时间
- `softirq`(si)：代表处理软中断CPU时间
- `steal`(st)：虚拟化中的值，当系统运行在虚拟机中，而物理机上还存在其他虚拟机，该值表示被其他虚拟机抢占的CPU时间。通常该值较大表示物理机CPU不足
- `guest`(guest)：代表通过虚拟化运行其他OS的时间，也就是虚拟机的CPU时间

> CPU使用率，就是除了空闲时间外的其他时间占总CPU时间的百分比

由于操作系统会不断更新/proc/stat中数据，所以只需要根据该文件就可以计算出一段时间内的cpu使用率情况。诸如`top`等命令就是这样CPU使用率的，该命令默认统计3秒内的使用率

当我们使用`top`命令查看使用率时，我们可以看到所有CPU的各个场景平均使用情况，以及各个进程的CPU总的使用情况，但无法看到各个进程在各个场景下的CPU使用情况。这是我们仍然可以通过`pidstat`命令来查看更加详细的使用情况，以此来定位进程级的CPU使用情况。

当定位到进程级之后，我们可以继续使用`perf`命令来查看哪个函数或者指令占用CPU时钟最多。

### 系统CPU使用率很高，但为什么找不到CPU的应用?

碰到常规问题无法解释CPU使用率情况时，即当我么通过`top`、`ps aux`等命令无法找到CPU使用率高的应用，但系统CPU使用率却很高的情况时，首先要想到有可能是**短时应用**导致的问题，比如以下两种情况：
- 应用里直接调用了其他二进制程序，这些程序通常运行时间比较短，通过top等工具也不容易发现
- 应用本身在不停地奔溃重启，而启动过程的资源初始化，很可能占用相当多的CPU

对于这类进程，我们可以用`pstree`或者`execsnoop`命令找到它们的父进程，再从父进程所在的应用入手，排查问题的根源

## 系统出现大量不可中断进程和僵尸进程

### 僵尸进程出现的原因

正常情况下，当一个进程创建了子进程之后，它应该通过系统调用`wait`或者`waitpid`等待子进程结束，然后回收子进程资源（分配给子进程的进程描述符等)；此外，当子进程在结束时，会向它的父进程发送`SIGCHILD`信号，所以父进程也可以注册该信号的处理函数，异步回收资源。

然而如果父进程没有按照上述两个方法做，或者子进程执行太快，父进程还没有来得及处理子进程的状态，子进程就已经退出。这时，子进程就变成了僵尸进程。

通常，僵尸进程持续的时间较短，在父进程回收它的资源后就会消亡；或者在父进程退出后，由init进程回收后也会消亡。

然而，如果父进程长时间存在，却不回收子进程资源，那么僵尸进程会一直存在。由于这些僵尸进程占用着进程描述符，长时间不清理，会使得无法创建新的进程。

### 僵尸进程的诊断

僵尸进程的排查相对容易，可以使用`pstree`找出父进程，然后查看父进程代码，检查`wait`或者`pidwait`的调用，或是`SIGCHLD`信号处理函数的注册即可

### 不可中断进程诊断

不可中断进程通常持续的时间很短，若长时间存在，那么出现问题的可能性较大。我们可以通过状态D的进程，使用`strace`找出系统调用函数，进一步分析。有时，当状态为D的进程为僵尸进程时，那么该命令就不工作了，此时，我们可以通过`perf`来找出调用函数。

## 软中断

中断其实是一种异步事件处理机制，可以提高系统的并发处理能力。为了减少对正常进程运行调度的影响，中断处理程序需要尽快完成。**中断关闭性质：当在响应上一次中断时，如果其他部分同时发出了中断信号，此时由于已经在响应其他中断了，那么该中断就会被丢失**

为了解决中断处理程序执行太长以及中断丢失的问题，Linux将中断处理分为两个阶段：
- 上半部：用来快速处理中断，它在中断禁止模式下运行，主要吹跟硬件紧密相关的或者时间敏感的工作，例如执行一些状态更新操作等。通常立即执行，执行非常快，从而解决中断丢失问题。这也就是常说的**硬中断**
- 下半部：延迟处理上半部未完成的工作，通常以**内核线程**方式运行。对应**软中断**

对于软中断，我们可以通过`cat /proc/softirqs`查看各种类型软中断在不同CPU上的累计运行次数。事实上，软中断以内核线程的方式**延迟执行**，每个CPU都对应一个软中断内核线程，由该线程来执行软中断程序。线程名字通常为`ksoftirqd/CPU编号`

此外，软中断不只包括了中断处理程序的下半部，一些内核自定义的事件也属于软中断。

### 诊断方法

当软中断事件过多时，会使得内核线程处理不及时，从而一起CPU使用率升高，进而引发网络收发延迟、调度缓慢等性能问题。例如网络频繁收发小包，会引起网络收发延迟。

当发现软中断占用较大CPU，并且是由`ksoftirqd/CPU编号`引起时，我们首先可以通过一下命令查找由什么类型软中断引起的
```shell
> watch -d cat /proc/softirqs
```
如果发现是由于网络引起时，我们可以继续使用以下命令查看具体的网络收发情况
```shell
> sar -n DEV 1
```
> sar 是一系统活动报告工具，既可以实时查看系统的当前活动，又可以配置保存和报告历史统计数据

最后，我们可以通过抓包分析，究竟是发送了什么包，进一步排查具体问题

```shell
> tcpdump -i eth0 -tcp port 80
```

## CPU性能问题分析技巧

[具体参考本文](https://time.geekbang.org/column/article/72685)

CPU性能指标
![cpu_checkpoint](/img/post/linux_performance/cpu-checkpoint.png)

从性能指标出发，查找对应排查工具
![index2tools](/img/post/linux_performance/index2tools.png)

从工具出发，查找该工具能干什么
![tools2index](/img/post/linux_performance/tools2index.png)

不同工具命令之间的关系
![tools_relation](/img/post/linux_performance/tools-relation.png)

## CPU性能优化思路

### 避免过早优化

性能优化是一个复杂的过程，造成性能问题可能是有很多因素共同引起的。只有我们了解清楚了，才应该进行。
- 能否确定优化有效？优化后，能提升多少性能
- 性能问题通常不是独立的，如果有多个性能问题同时发生，应该优化哪一个？
- 提升性能问题的方法有时不止一种，如何选择？

**如何评估性能优化效果**

1. 确定性能的量化指标
2. 测试优化前的性能指标
3. 测试优化后的性能指标

通常建议考虑 **应用程序** 与 **系统资源** 两个维度来确定性能指标。如对于应用程序来说，可以考虑吞吐量、请求延迟等；系统资源角度的，可以考虑CPU使用率等

### CPU优化

应用程序优化建议：
- 编译器优化
- 算法优化
- 异步处理。避免轮询造成的CPU开销
- 多线程代替多进程
- 善用缓存

系统优化建议：
- CPU绑定。把进程绑定到一个或者多个CPU上，提高缓存命中率，减少跨CPU调度带来的上下文切换问题
- CPU独占
- 优先级调整。适当降低非核心应用的优先级
- 为进程栓塞制资源限制。使用Linux cgroups设置进程的CPU上限，防止由于某个应用自身问题，而耗尽系统资源
- 中断负载均衡。将中断处理过程自动负载均衡到多个CPU上

# Linux内存

虚拟空间分为内核空间与用户空间，以32位系统举例，如下图所示。
![virtual_memory](/img/post/linux_performance/virtual-memory.png)

用户空间从低地址到高地址分别是五种不同内存段
1. 只读段，包括**代码**与**常量**
2. 数据段，包括**全局变量**等
3. 堆，包括动态分配的内存，从低地址开始向上增长
4. 文件映射段，包括动态库，共享内存等，从高地址向下减小
5. 栈，包括局部变量和函数调用的上下文等。栈的大小是固定的，一般是8M

## 动态内存分配

`malloc`是C标准库提供的内存分配函数，对应到系统调用上，有两种实现方式，即 `brk()`和 `mmap()`
- `brk`。对于小块内存（小于128K），C标准库使用brk来分配，也即移动堆顶指针位置来分配。这些内存释放后不会立刻归还系统，而是被缓存起来，这样就可以重复使用。缓存的一个好处在于，可以不会频繁的发生缺页中断，提供内存访问效率。不足之处在于频繁的分配与释放会造成内存碎片。所以Linux提供了其他补偿机制，例如stab来管理小内存
- `mmap`。对于大块内存（大于128K），直接使用mmap在 **文件映射段** 中找出一块空闲内存分配。使用该方式分配的内存在使用完毕后会立即归还系统。所以如果频繁发生，会带来大量的缺页中断。

当内存资源紧张时，系统会通过一系列来回收内存：
- 回收缓存，例如使用LRU算法，回收最近最少使用的内存页面
- 使用SWAP
- 杀死进程，报OOM(Out of Memory)

Linux为每个进程设置了oom_score，如果该值越高，代表该进程越有可能被杀掉。当然，我们可以手动设置进行人为干预。

## 内存查看工具

#### free

显示的两行分别表示物理内存Mem和交换分区Swap的使用情况。

- total。总内存大小
- used。已使用内存大小，包括共享内存
- free。未使用内存
- shared。共享内存，指所有进程共同使用的内存、加载的动态链接库以及程序代码段
- buff/cache。缓存和缓冲区大小
- available。新进程可用内存大小。不仅包含未使用内存，还包括可回收的缓存，所以一般比未使用内存大

#### top/ps

- VIRT。进程虚拟内存大小。 **注意：进程申请内存后，并不会立即分配，只有首次访问时，才会发生中断进行分配**。该值值申请的内存大小，即使没有真正分配也会计算在内。
- RES。常驻内存大小，也就是进程实际使用的物理内存大小，但不包括swap和共享内存
- SHR。共享内存大小
- %MEM。进程使用的物理内存占系统总内存百分比。

## Buffer&Cache

通过free，我们可以看到缓存与缓冲区大小之和。通过 `vmstat`则可以分别看到各自的大小。

Buffer和cache分别缓存 **磁盘和文件系统** 读写数据

- buffer既可以用作“将要写入 **磁盘** 数据的缓存 ”，也可以用作“从 **磁盘** 读取数据的缓存”
- cache既可以用作“从 **文件** 读取数据的页缓存”，也可以用作“ 写 **文件** 的页缓存”

总之，buffer用于对磁盘读写时的缓存，而cache用于对于文件系统的读写缓存。默认情况下，我们都是基于文件系统去读写，所以看到的大多数都是cache变化比较明显。

因此，page cache是文件系统的产物。理论上，一个文件读首先到buffer，然后再到page cache。所以在老的linux系统上，buffer和cache是分开的，这样的话，对于文件数据会被缓存两次，造成性能不高。因此，在之后的linux版本中，两者统一了，对于文件，page cache直接指向buffer，即只需一次操作。

## 缓存诊断

> 缓存命中率通常用来衡量缓存使用的好坏

### 缓存诊断工具

#### bcc

由于linux没有直接提供查询缓存命中率这些接口，而bcc软件包基于内核的eBPF机制，来跟踪内核管理缓存，并输出缓存使用和命中情况。其提供了两个命令
- cachestat:提供了整个操作系统缓存的读写命中情况。但注意该命令，没有统计直接IO的缓存命中情况
- cachetop:提供了每个进程的缓存命中情况

这两个命令的输出的命中情况都以 `页`为单位，假设每个页大小为4KB，然后按照统计时间间隔，计算得到每秒从缓存读写字节速率。有时使用读写字节速率更能形象展示出缓存使用情况

#### pcstat

- pcstat: 查看**某个文件**在内存中的缓存大小以及缓存比例。

## 内存泄漏

> 当在堆区或者文件映射区申请了空间，但未归还，则会出现内存泄漏。使得其他进程无法申请到内存而失败

**所以记得要在申请了空间使用完毕后，一定要及时回收掉**

我们可以使用bcc软件包中的 `memleak`工具查看指定进程的内存申请情况，以及输出应用程序的调用栈，定位内存分配位置。

## Swap

### 内存回收

#### 文件页回收
内存紧张时，系统可以释放掉可以回收的内存，比如缓存和缓冲区就属于可回收内存。他们在内存管理中被称为 `文件页`。

大部分文件页都可以直接回收，以后有需要时，再从磁盘重新读取即可。对于那些脏页，需要先写入磁盘，然后才能进行内存释放。

#### 匿名页

> 匿名页指由应用程序动态分配的堆内存。

考虑到这些动态分配的内存还有可能被访问到，不能直接释放。所以通常使用swap来存放这些匿名页

### Swap原理

> 简单来说，swap就是把一块磁盘空间或者一个本地文件当做内存使用。

包括换出与换入两个过程
- 换出。把进程暂时不用的内存数据存储到磁盘中，然后释放这些数据占用的空间
- 换入。需要再次访问那些被换出的内存的时候，把它们从磁盘读到内存中来

### 内存回收时间

当有新的大块内存分配请求到来时，但是剩余内存不足，这个时候系统就要回收一部分内存（例如缓存缓冲区）。这个过程通常被称为 `直接内存回收`

此外，还有一个专门内核线程用来定期回收内存，即 `kswapd0`，其定义了三个阙指。
![kswapd0](/img/post/linux_performance/kswapd0.png)

- 剩余内存小于 `pages_min`，说明进程可用内存都耗尽了，只有内核才可以分配内存。
- 剩余内存在 `pages_min`与 `pages_low`之间，说明内存压力比较大，剩余内存不多了。这是该线程会执行内存回收，直到剩余内存大于高阙值为止
- 剩余内存在 `pages_low`与 `pages_high`之间，说明内存有一定压力，但还可以满足新内存申请
- 剩余内存大于 `pages_high`，没有内存压力

因此，一旦内存小于 `pages_low`时，就会触发内存回收。该值可以 `/proc/sys/vm/min_free_kbytes`进行设置

### NUMA架构

> NUMA： Non-Uniform Memory Access。该架构下，多个处理器被划分到不同Node上，且每个Node都拥有自己的本地内存空间

可以通过以下命令查看处理器在Node上分布情况，以及每个Node的内存使用情况
```shell
> numactl --hardware
```

此外，也可以通过以下命令查看各个Node的内存使用情况，包括活跃与非活跃的的文件页或者匿名页数量
```shell
> cat /proc/zoneinfo
```

当某个node内存不足时，系统可以从其他Node寻找空闲内存，或者也可以从本地内存中回收内存。可以通过 `/proc/sys/vm/zone_reclaim_mode`来调整。当设置为只能从本地内存回收时，也就可能会出现，系统明明有空闲内存，也会出现swap。

事实上当前，我们一般只配置一个Node，即所有cpu都在一个node上运行。虽然NUMA意图在于做到系统资源隔离，但是当前虚拟化docker的流行，而node与node之间访问更耗时，所以一般不开启。

### swappiness

上面说了，内存回收有两种方式，一种为直接内存回收，只要针对缓存，用于文件页；另一种为swap，用于匿名页。

关于系统使用哪一种，我们可以通过 `/proc/sys/vm/swappiness`选项，来调整使用swap的积极程度。其配置范围是一个0-100值，数值越大，越积极使用swap，也就是越倾向于回收匿名页。 **需要注意的是，该值只是一个倾向值，并不保证**

### swap使用建议

由于swap机制会使用到较慢的磁盘IO，从而引起性能降低，在实际系统中，我们通常关闭。例如在一些大型Java应用中，如ES，要求关闭swap，这是因为java的垃圾回收机制要在堆上遍历对象，如果这些对象被换出到磁盘上了，那么势必会影响到垃圾回收时间，从而引起应用性能下降。

## 内存总结

![memory_indices](/img/post/linux_performance/memory-indices.png)

- 主缺页异常。需要磁盘IO介入的缺页异常
- 次缺页异常。可以直接从物理内存中分配的缺页异常。

![indices2tools](/img/post/linux_performance/indices2tools.png)

![tools2indices](/img/post/linux_performance/tools2indices.png)

![procedure](/img/post/linux_performance/procedure.png)

分析流程
1. 先用free和top，查看系统整体的内存使用情况
2. 再用vmstat和pidstat，查看一段时间的趋势，从而判断内存问题类型
3. 最后进行详细分析，比如内存分配分析、缓存/缓冲区分析、具体进程的内存使用分析等

# 文件系统

> Linux中一切皆文件

文件系统为每个文件都分配两个数据结构， `索引节点` 和 `目录项`。用来记录文件的元信息和目录结构。

- 索引节点inode。用来记录文件的元数据，比如inode编号、文件大小、访问权限、修改日期、数据位置等。**索引节点和文件一一对应，是每个文件的唯一标识**，此外，索引节点和文件内容一样，都会被持久化存储到磁盘中。 **所以索引节点同样占用磁盘空间**
- 目录项dentry。用来记录文件的名字、索引节点指针以及其他目录项的关联关系（例如父子关系）。多个关联的目录项，就成了文件系统的目录结构。 **目录项和索引节点的关系是多对一，即一个文件可以有多个别名**。不过，不同于索引节点， **目录项是内和维护的一个内存数据结构，不会持久化到磁盘，只会存在于缓存中**

**软链接与硬链接**

- 硬链接为文件创建的别名，会对应不同的目录项，但是这些目录项本质上海市链接到同一个文件。所以它们的索引节点相同
- 软链接则真正创建了一个文件，只不过这个文件比较特殊，其内容指向了要被链接的文件的目录项。所以软链接创建的文件与原始文件有着不同的inode

![dentry&inode](/img/post/linux_performance/dentry-inode.png)

- 这里，磁盘读写以扇区管理，然而扇区只有512B大小，如果每次读写这么小的单位，会使得效率较低。所以文件系统以块为单位管理数据，常见逻辑块大小为4KB，即连续8个扇区组成
- 如前所述，目录项本身就在于内存中，而索引节点开始存在于磁盘中。为了协调慢速磁盘与快速CPU的性能差异，索引节点在系统运行后， **也会被缓存到内存中，加速文件访问**
- 磁盘在执行文件系统格式化时，会被分成三个区域，超级块（存储整个文件系统状态）；索引节点区（用来存储索引节点）；数据块（存储文件数据）

### 虚拟文件系统

为了支持不同的文件系统，例如EXT3，EXT4等，Linux内核在用户和文件系统中间，又引入了一个抽象层，也就是虚拟文件系统VFS。VFS定义了一组所有文件系统都支持的数据结构和标准接口。用户进程只需要和VFS接口交互就可以了，不用关心底层各种文件系统实现细节。

![vfs](/img/post/linux_performance/vfs.png)

可以看到，在VFS下方，Linux支持各种文件系统，主要可以分为三类：
- 基于磁盘的文件系统。也就是把数据直接存储在计算机本地挂载的磁盘中。常见的有EXT4、XFS、OverlayFS
- 基于内存的文件系统。这类系统，不需要任何磁盘分配存储空间，但会占用内存。/proc与/sys文件系统也属于这一类，主要向用户空间导出层次化内核对象
- 网络文件系统。如NFS

这些文件系统，要先挂载到VFS目录树的某个子目录（称为挂载点）,然后才能访问其中的文件。例如在安装系统的时候，要设置选择磁盘，格式化成指定格式，然后挂载到某个目录，这里的目录就是VFS中的目录

### 文件系统IO

把文件系统挂载到挂载点后，你就能通过挂载点，再去访问它管理的文件了。VFS提供了一组标准的文件访问接口，如open、read、write等，这些接口以系统调用的方式提供给应用程序使用。

**文件读写的四种分类**

#### 缓冲IO与非缓冲IO

这里的缓冲指的是用户空间程序自己开辟的缓冲。事实上，这种方式会带来很多的CPU拷贝开销

#### 直接与非直接IO

##### IO写入调用链

![page_cache](/img/post/linux_performance/page-cache.png)

https://www.cnblogs.com/zhaoyl/p/5901680.html

1. 数据先写入到page cache
2. 内核有pdflush线程在不停的检测脏页，判断是否要写回到磁盘中。把需要写回的页提交到IO队列——即IO调度队列。由IO调度队列调度策略调度何时写回（这里就是书本上介绍的磁盘调度，因为磁头只会来回移动）
3.  从IO队列出来后，就到了驱动层(当然内核中有更多的细分层，这里忽略掉)，驱动层通过DMA，将数据写入磁盘cache。
4. 至于磁盘cache时候写入磁盘介质，那是磁盘控制器自己的事情。可以调用fsync函数吧。可以确定写到磁盘上了。

指是否理由操作系统提供的页缓存Page Cache。
- 直接IO。指跳过OS的页缓存，直接跟文件系统交互操作文件。如在系统调用中，指定 `O_DIRECT`标志
- 非直接IO。文件读写时，会先经过page cache

**注意：直接IO与非直接IO本质上还是和文件系统交互**，如果在数据库等场景下，可以跳过文件系统直接读写磁盘， 即**裸IO, Raw IO**

[如何控制裸IO](https://stackoverflow.com/questions/3520459/direct-access-to-hard-disk-with-no-fs-from-c-program-on-linux)

**O_DIRECT与Raw IO区别**
- O_DIRECT是基于文件系统的，其操作对象是文件句柄，其操作是基于inode和数据块的。一般基于O_DIRECT来设计优化自己的文件模块，是不满page cache和调度策略，自己在应用层实现这些，来制定自己特有的业务特色文件读写。但是写出来的东西是ext3文件，该磁盘卸下来，mount到其他任何linux系统上，都可以查看。
- Raw IO则没有文件系统概念，操作的是扇区号，操作对象是扇区。而基于RAW设备的设计系统，一般是不满现有ext3的诸多缺陷，设计自己的文件系统。自己设计文件布局和索引方式。举个极端例子：把整个磁盘做一个文件来写，不要索引。这样没有inode限制，没有文件大小限制，磁盘有多大，文件就能多大。这样的磁盘卸下来，mount到其他linux系统上，是无法识别其数据的。

#### 阻塞与非阻塞IO

#### 同步与异步IO

## IO性能观测

### 容量

#### df

查看磁盘空间，不过默认显示的是文件数据占用的，未记录索引节点占用的磁盘空间。可以使用 `df -i`显示。当你发现索引节点空间不足，但磁盘空间充足时，很可能是过多小文件导致的

### 缓存

从上图可以看到，OS中有page cache、目录项缓存与索引节点缓存。通过free查得到是cache与buffer之和。而cache又是page cache与slab用到的内存之和。而slab即目录缓存与索引节点缓存之和。为了区分两种缓存的大小，我们可以通过 `/proc/slabinfo`查看

```shell
> cat /proc/slabinfo | grep -E '^#|dentry|inode'
```
dentry行表示目录项缓存，inode_cache行表示VFS索引节点缓存。其余为各种文件系统的索引节点缓存。

更简单的，我们可以使用 `slabtop`找到占用内存最多的缓存类型

## 磁盘IO

### 磁盘名称
在Linux中，由于不同类型硬盘有不同的接口，所以为每个接口分配不同的设备名称。例如IDE设备会分配一个hd前缀的设备名；SCSI和SATA设备会分配一个sd前缀的设备名。

此外，如果有多块同类型的磁盘，就会按照a、b、c等的字幕顺序来代表第几块。

再者，我们可以为磁盘进行分区，Linux使用数字编号来标识第几个分区。例如为磁盘 `/dev/sda`创建两个分区，则有： `/dev/sda1` 和 `/dev/sda2`

### IO栈

![io_stack](/img/post/linux_performance/io-stack.png)

关于通用块层功能：
- 与虚拟文件系统功能类似。向上，为文件系统和应用程序，提供访问块设备标准统一接口；向下，把各种异构的磁盘设备抽象为统一块设备，并提供统一框架来管理这些设备的驱动程序
- 通用块层还会给文件系统和应用程序发来的IO请求排队，并通过重新排序、请求合并等方式，提高磁盘读写效率。

### 磁盘性能指标

- 使用率。指磁盘处理IO的时间百分比。过高的使用率，通常意味着磁盘IO存在性能瓶颈
- 饱和度。指磁盘处理IO的繁忙程度。饱和度为100%时，无法接受新的IO请求
- IOPS。每秒的IO请求数
- 吞吐量。指每秒的IO请求大小
- 响应时间。指IO请求发出到响应的时间间隔。

## IO总结

![io_indices](/img/post/linux_performance/io-indices.png)

![ioindices2tools](/img/post/linux_performance/ioindices2tools.png)

![tools2ioindices](/img/post/linux_performance/tools2ioindices.png)

IO问题发现处理流程
1. 先用iostat发现磁盘IO性能瓶颈
2. 再借助pidstat，定位出导致瓶颈的进程
3. 随后分析进程的IO行为
4. 集合应用程序原理，分析IO来源

![io_procedure](/img/post/linux_performance/io-procedure.png)

## 磁盘IO性能优化

### 应用程序优化

- 可以用追加写代替随机写，减少寻址开销，加快IO写速度
- 可以借助缓存IO，充分利用系统缓存，降低实际IO次数
- 可以在应用程序内部构建自己的缓存。例如使用redis这样的外部缓存；或者如C标准库提供的fopen、fread等库函数，都会利用标准库的缓存，减少系统调用与磁盘操作
- 在需要频繁读写同一块磁盘空间时，可以用mmap代替 read/write，减少内存拷贝次数
- 在多个应用程序共享相同磁盘时，为了保证IO不被某个应用完全占用，推荐使用cgroups的IO子系统，来隔离进程资源
- 在使用CFQ调度器时，可以用ionice来调整进程的IO调度优先级，特别是提高核心应用的IO优先级


### 文件系统优化

- 根据实际负载场景的不同，选择最适合的文件系统
- 在选好文件系统后，还可以进一步优化文件系统的配置选项
- 优化文件系统缓存。如优化pdflush脏页的刷新频率；优化内核的回收目录项缓存和索引节点缓存的倾向
- 在不需要持久化时，还可以用内存文件系统tmpfs，以获得更好的IO性能

### 磁盘优化

- SSD代替HDD
- 使用RAID，提高数据可靠性，也提高数据访问性能
- 选择合适的IO调度算法
- 对应用程序的数据，进行磁盘级别隔离。比如为日志、数据库等IO压力比较重的应用，配置单独的磁盘
- 在顺序读比较多的场景中，可以增大磁盘的预读数据
- 可以优化内核块设备IO选项。如可以调整磁盘队列长度，适当增大队列长度，可以提升磁盘吞吐量，但同时也会导致IO延迟增大
